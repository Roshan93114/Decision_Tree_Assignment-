{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiVJ8GpbkICh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "                           Questions And Answers"
      ],
      "metadata": {
        "id": "ooisP91DlRxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "Answer:A Decision Tree is a supervised machine learning algorithm used for both classification and regression, but it is most commonly applied to classification problems.\n",
        "\n",
        "‚≠ê What is a Decision Tree?\n",
        "\n",
        "A Decision Tree is a flowchart-like structure where:\n",
        "\n",
        "Each internal node represents a feature (attribute).\n",
        "\n",
        "Each branch represents a decision (rule) based on that feature.\n",
        "\n",
        "Each leaf node represents an output or class label.\n",
        "\n",
        "It splits the dataset into smaller subsets based on the most significant feature at each step, forming a tree-like model of decisions.\n",
        "\n",
        "‚≠ê How Does a Decision Tree Work in Classification?\n",
        "\n",
        "Here‚Äôs the step-by-step process:\n",
        "\n",
        "1. Select the Best Feature to Split\n",
        "\n",
        "The algorithm chooses the feature that best separates the data into classes.\n",
        "\n",
        "Common metrics used:\n",
        "\n",
        "Gini Impurity (CART algorithm)\n",
        "\n",
        "Entropy / Information Gain (ID3, C4.5)\n",
        "\n",
        "Gain Ratio\n",
        "\n",
        "The goal is to pick the feature that provides the purest split.\n",
        "\n",
        "2. Split the Data\n",
        "\n",
        "The dataset is divided into subsets based on the chosen feature‚Äôs values.\n",
        "\n",
        "Example:\n",
        "If feature = ‚ÄúAge‚Äù, the split may be:\n",
        "\n",
        "Age < 30\n",
        "\n",
        "30 ‚â§ Age < 50\n",
        "\n",
        "Age ‚â• 50\n",
        "\n",
        "3. Repeat Recursively\n",
        "\n",
        "The process continues on each subset:\n",
        "\n",
        "Pick the best feature\n",
        "\n",
        "Split the data\n",
        "\n",
        "Create branches\n",
        "\n",
        "This continues until:\n",
        "\n",
        "All data in a node belongs to the same class (pure node), or\n",
        "\n",
        "No more features are left.\n",
        "\n",
        "4. Form Leaf Nodes\n",
        "\n",
        "Each final node (leaf) represents a class label.\n",
        "\n",
        "Example:\n",
        "\n",
        "If Age < 30 AND Income = High ‚Üí Class = ‚ÄúBuys Computer = Yes‚Äù\n",
        "\n",
        "‚≠ê Example (Simplified)\n",
        "\n",
        "Suppose we classify whether a student passes an exam based on:\n",
        "\n",
        "Study Hours\n",
        "\n",
        "Attendance\n",
        "\n",
        "The tree may look like:\n"
      ],
      "metadata": {
        "id": "kKFcMiqmlV8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "            Study Hours?\n",
        "           /            \\\n",
        "       < 3 hrs          ‚â• 3 hrs\n",
        "       /                    \\\n",
        "   Attendance?              Pass\n",
        "   /        \\\n",
        " Low       High\n",
        " Fail      Pass\n"
      ],
      "metadata": {
        "id": "Rjem1bgNljCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚≠ê Advantages\n",
        "\n",
        "Easy to visualize and interpret\n",
        "\n",
        "Requires little data preprocessing\n",
        "\n",
        "Works well with both numerical and categorical data\n",
        "\n",
        "‚≠ê Disadvantages\n",
        "\n",
        "Prone to overfitting\n",
        "\n",
        "Small changes in data can create a different tree\n",
        "\n",
        "May become complex for large datasets"
      ],
      "metadata": {
        "id": "ejvkoIZYllgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "Answer:‚≠ê Impurity Measures in Decision Trees\n",
        "\n",
        "When building a decision tree, the goal is to split the data into subsets that are as pure as possible ‚Äî meaning each subset contains mostly or entirely one class.\n",
        "\n",
        "Two commonly used impurity measures are:\n",
        "\n",
        "Gini Impurity (used in CART)\n",
        "\n",
        "Entropy (used in ID3, C4.5)\n",
        "\n",
        "‚≠ê 1. Gini Impurity\n",
        "Definition\n",
        "\n",
        "Gini Impurity measures how often a randomly chosen sample would be incorrectly classified if it were labeled according to the class distribution in the node.\n",
        "\n",
        "Formula\n",
        "\n",
        "For a node with k classes:\n",
        "\n",
        "ùê∫\n",
        "ùëñ\n",
        "ùëõ\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëò\n",
        "ùëù\n",
        "ùëñ\n",
        "2\n",
        "Gini=1‚àí\n",
        "i=1\n",
        "‚àë\n",
        "k\n",
        "\t‚Äã\n",
        "\n",
        "p\n",
        "i\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "ùëù\n",
        "ùëñ\n",
        "p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        " = probability (proportion) of class i in the node.\n",
        "\n",
        "Interpretation\n",
        "\n",
        "0 ‚Üí perfectly pure node (all samples belong to one class)\n",
        "\n",
        "Higher Gini ‚Üí higher impurity (more mixed classes)\n",
        "\n",
        "Example\n",
        "\n",
        "If a node has:\n",
        "\n",
        "80% class A\n",
        "\n",
        "20% class B\n",
        "\n",
        "ùê∫\n",
        "ùëñ\n",
        "ùëõ\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "(\n",
        "0.8\n",
        "2\n",
        "+\n",
        "0.2\n",
        "2\n",
        ")\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "(\n",
        "0.64\n",
        "+\n",
        "0.04\n",
        ")\n",
        "=\n",
        "0.32\n",
        "Gini=1‚àí(0.8\n",
        "2\n",
        "+0.2\n",
        "2\n",
        ")=1‚àí(0.64+0.04)=0.32\n",
        "‚≠ê 2. Entropy (Information Gain)\n",
        "Definition\n",
        "\n",
        "Entropy measures the uncertainty or disorder in a node.\n",
        "\n",
        "Formula\n",
        "ùê∏\n",
        "ùëõ\n",
        "ùë°\n",
        "ùëü\n",
        "ùëú\n",
        "ùëù\n",
        "ùë¶\n",
        "=\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëò\n",
        "ùëù\n",
        "ùëñ\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "(\n",
        "ùëù\n",
        "ùëñ\n",
        ")\n",
        "Entropy=‚àí\n",
        "i=1\n",
        "‚àë\n",
        "k\n",
        "\t‚Äã\n",
        "\n",
        "p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "log\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "(p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "Interpretation\n",
        "\n",
        "0 ‚Üí pure node\n",
        "\n",
        "Higher Entropy ‚Üí more disorder (classes evenly mixed)\n",
        "\n",
        "Maximum entropy occurs when classes are equally likely (50/50 split)\n",
        "\n",
        "Example\n",
        "\n",
        "Same example:\n",
        "\n",
        "80% class A\n",
        "\n",
        "20% class B\n",
        "\n",
        "ùê∏\n",
        "ùëõ\n",
        "ùë°\n",
        "ùëü\n",
        "ùëú\n",
        "ùëù\n",
        "ùë¶\n",
        "=\n",
        "‚àí\n",
        "(\n",
        "0.8\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "0.8\n",
        "+\n",
        "0.2\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "0.2\n",
        ")\n",
        "‚âà\n",
        "0.72\n",
        "Entropy=‚àí(0.8log\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "0.8+0.2log\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "0.2)‚âà0.72\n",
        "‚≠ê How They Impact Splits in a Decision Tree\n",
        "\n",
        "Decision Trees try to reduce impurity with each split.\n",
        "\n",
        "‚úîÔ∏è Splitting Criterion\n",
        "\n",
        "Gini ‚Üí chooses split that minimizes Gini impurity\n",
        "\n",
        "Entropy (Information Gain) ‚Üí chooses split that maximizes Information Gain\n",
        "\n",
        "ùêº\n",
        "ùê∫\n",
        "=\n",
        "ùê∏\n",
        "ùëõ\n",
        "ùë°\n",
        "ùëü\n",
        "ùëú\n",
        "ùëù\n",
        "ùë¶\n",
        "(\n",
        "ùëù\n",
        "ùëé\n",
        "ùëü\n",
        "ùëí\n",
        "ùëõ\n",
        "ùë°\n",
        ")\n",
        "‚àí\n",
        "‚àë\n",
        "ùëõ\n",
        "ùëê\n",
        "‚Ñé\n",
        "ùëñ\n",
        "ùëô\n",
        "ùëë\n",
        "ùëõ\n",
        "ùëù\n",
        "ùëé\n",
        "ùëü\n",
        "ùëí\n",
        "ùëõ\n",
        "ùë°\n",
        "ùê∏\n",
        "ùëõ\n",
        "ùë°\n",
        "ùëü\n",
        "ùëú\n",
        "ùëù\n",
        "ùë¶\n",
        "(\n",
        "ùëê\n",
        "‚Ñé\n",
        "ùëñ\n",
        "ùëô\n",
        "ùëë\n",
        ")\n",
        "IG=Entropy(parent)‚àí‚àë\n",
        "n\n",
        "parent\n",
        "\t‚Äã\n",
        "\n",
        "n\n",
        "child\n",
        "\t‚Äã\n",
        "\n",
        "\t‚Äã\n",
        "\n",
        "Entropy(child)\n",
        "\n",
        "Better splits produce purer child nodes, meaning:\n",
        "\n",
        "Samples of the same class get grouped together\n",
        "\n",
        "Tree becomes more meaningful and accurate\n",
        "\n",
        "‚≠ê Differences Between Gini and Entropy in Practice\n",
        "Feature\tGini Impurity\tEntropy\n",
        "Calculation\tSimple, faster\tMore complex (logarithm)\n",
        "Preferred By\tCART algorithm (default in scikit-learn)\tID3, C4.5\n",
        "Splitting Behavior\tSlight bias toward large class\tMore sensitive to rare classes\n",
        "Performance\tOften similar results\tOften similar results\n"
      ],
      "metadata": {
        "id": "WRGh4vt-lprH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "Answer:‚≠ê Pre-Pruning vs. Post-Pruning in Decision Trees\n",
        "\n",
        "Decision Trees tend to overfit if allowed to grow fully.\n",
        "Pruning helps control size and improve generalization.\n",
        "\n",
        "üîµ 1. Pre-Pruning (Early Stopping)\n",
        "Definition\n",
        "\n",
        "Pre-pruning stops the tree from growing too deep during the training process.\n",
        "You set constraints such as:\n",
        "\n",
        "Maximum depth (max_depth)\n",
        "\n",
        "Minimum samples to split (min_samples_split)\n",
        "\n",
        "Minimum samples per leaf (min_samples_leaf)\n",
        "\n",
        "Maximum number of leaves (max_leaf_nodes)\n",
        "\n",
        "How it works\n",
        "The tree stops splitting when:\n",
        "\n",
        "Further splits do not significantly reduce impurity\n",
        "or\n",
        "\n",
        "A predefined constraint is reached.\n",
        "\n",
        "Practical Advantage\n",
        "\n",
        "‚úÖ Faster training ‚Äî prevents the tree from becoming overly large, saving computation time and memory.\n",
        "\n",
        "üîµ 2. Post-Pruning (Cost-Complexity Pruning / Reduced Error Pruning)\n",
        "Definition\n",
        "\n",
        "Post-pruning allows the tree to grow fully, and then prunes back nodes that do not improve accuracy on a validation set.\n",
        "\n",
        "How it works\n",
        "\n",
        "Grow a full decision tree.\n",
        "\n",
        "Evaluate performance on validation data.\n",
        "\n",
        "Remove branches that do not increase accuracy or reduce error.\n",
        "\n",
        "Simplify the tree while maintaining or improving generalization.\n",
        "\n",
        "Practical Advantage\n",
        "\n",
        "‚úÖ Better generalization ‚Äî pruning removes overfitted branches, resulting in a simpler and more accurate model on unseen data."
      ],
      "metadata": {
        "id": "CyCOxc_8l_Nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "Answer:Information Gain is a key concept used in Decision Trees‚Äîespecially those based on Entropy (like ID3 and C4.5)‚Äîto determine which feature provides the best split at each step in the tree.\n",
        "\n",
        "‚≠ê What is Information Gain?\n",
        "\n",
        "Information Gain measures how much uncertainty (entropy) is reduced after splitting the dataset based on a particular feature.\n",
        "\n",
        "Formula\n",
        "ùêº\n",
        "ùê∫\n",
        "=\n",
        "ùê∏\n",
        "ùëõ\n",
        "ùë°\n",
        "ùëü\n",
        "ùëú\n",
        "ùëù\n",
        "ùë¶\n",
        "(\n",
        "ùëù\n",
        "ùëé\n",
        "ùëü\n",
        "ùëí\n",
        "ùëõ\n",
        "ùë°\n",
        ")\n",
        "‚àí\n",
        "‚àë\n",
        "ùëõ\n",
        "ùëê\n",
        "‚Ñé\n",
        "ùëñ\n",
        "ùëô\n",
        "ùëë\n",
        "ùëõ\n",
        "ùëù\n",
        "ùëé\n",
        "ùëü\n",
        "ùëí\n",
        "ùëõ\n",
        "ùë°\n",
        "√ó\n",
        "ùê∏\n",
        "ùëõ\n",
        "ùë°\n",
        "ùëü\n",
        "ùëú\n",
        "ùëù\n",
        "ùë¶\n",
        "(\n",
        "ùëê\n",
        "‚Ñé\n",
        "ùëñ\n",
        "ùëô\n",
        "ùëë\n",
        ")\n",
        "IG=Entropy(parent)‚àí‚àë\n",
        "n\n",
        "parent\n",
        "\t‚Äã\n",
        "\n",
        "n\n",
        "child\n",
        "\t‚Äã\n",
        "\n",
        "\t‚Äã\n",
        "\n",
        "√óEntropy(child)\n",
        "\n",
        "Where:\n",
        "\n",
        "Entropy(parent) = impurity before the split\n",
        "\n",
        "Entropy(child) = impurity of each subset after the split\n",
        "\n",
        "ùëõ\n",
        "ùëê\n",
        "‚Ñé\n",
        "ùëñ\n",
        "ùëô\n",
        "ùëë\n",
        "ùëõ\n",
        "ùëù\n",
        "ùëé\n",
        "ùëü\n",
        "ùëí\n",
        "ùëõ\n",
        "ùë°\n",
        "n\n",
        "parent\n",
        "\t‚Äã\n",
        "\n",
        "n\n",
        "child\n",
        "\t‚Äã\n",
        "\n",
        "\t‚Äã\n",
        "\n",
        " = weighted proportion of each subset\n",
        "\n",
        "A higher Information Gain means the split creates purer (more homogeneous) child nodes.\n",
        "\n",
        "‚≠ê Why Is Information Gain Important for Choosing the Best Split?\n",
        "‚úîÔ∏è 1. It chooses the most informative feature\n",
        "\n",
        "Features that reduce uncertainty the most help the tree create purer nodes.\n",
        "Higher Information Gain = better separation of classes.\n",
        "\n",
        "‚úîÔ∏è 2. Leads to more accurate and meaningful splits\n",
        "\n",
        "A split that increases purity improves the decision tree's classification ability.\n",
        "\n",
        "‚úîÔ∏è 3. Helps prevent unnecessary splits\n",
        "\n",
        "If a feature provides little or no Information Gain, the tree will not split on it‚Äîkeeping the model simple.\n",
        "\n",
        "‚úîÔ∏è 4. Drives the greedy learning process of Decision Trees\n",
        "\n",
        "Decision Trees build top-down using recursive binary or multiway splits, choosing at each step the feature with the maximum Information Gain.\n",
        "\n",
        "‚≠ê Intuition With an Example\n",
        "\n",
        "If a feature (e.g., ‚ÄúStudy Hours‚Äù) divides students almost perfectly into ‚ÄúPass‚Äù and ‚ÄúFail,‚Äù the child nodes will have low entropy and the Information Gain will be high.\n",
        "\n",
        "If another feature (e.g., ‚ÄúShoes Color‚Äù) does not correlate with passing or failing, both child nodes will still be mixed ‚Üí low Information Gain.\n",
        "\n",
        "The tree will choose Study Hours for splitting."
      ],
      "metadata": {
        "id": "scl_66bYmhul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "Answer:\n",
        "2\n",
        "Dataset Info:\n",
        "‚óè Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "‚óè Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "Answer:-1. Common Real-World Applications of Decision Trees\n",
        "a) Classification Tasks\n",
        "\n",
        "Medical diagnosis\n",
        "Classifying diseases based on symptoms and test results.\n",
        "\n",
        "Customer churn prediction\n",
        "Predicting whether a customer will leave a service.\n",
        "\n",
        "Fraud detection\n",
        "Identifying fraudulent vs. legitimate transactions.\n",
        "\n",
        "Email spam filtering\n",
        "Categorizing emails as spam or not spam.\n",
        "\n",
        "Iris Dataset (example)\n",
        "Classifying iris flowers into species (Setosa, Versicolor, Virginica) using features like sepal length, petal width, etc.\n",
        "\n",
        "b) Regression Tasks\n",
        "\n",
        "Real-estate price prediction\n",
        "Estimating house prices based on features such as rooms, area, etc.\n",
        "Boston Housing Dataset is a common example.\n",
        "\n",
        "Demand forecasting\n",
        "Predicting sales or inventory needs.\n",
        "\n",
        "Risk assessment\n",
        "Predicting loan default risk or insurance claim amounts.\n",
        "\n",
        "2. Main Advantages of Decision Trees\n",
        "\n",
        "Easy to understand and visualize\n",
        "Trees resemble human decision-making and can be interpreted without statistical knowledge.\n",
        "\n",
        "Handles both numerical and categorical data\n",
        "Flexible for diverse datasets, such as the Iris dataset (numerical) or mixed-type data.\n",
        "\n",
        "Requires little preprocessing\n",
        "No need for feature scaling or normalization.\n",
        "\n",
        "Works for both classification and regression tasks\n",
        "Suitable for tasks like Iris classification and Boston Housing regression.\n",
        "\n",
        "Captures nonlinear relationships\n",
        "Splits can model complex decision boundaries.\n",
        "\n",
        "3. Limitations of Decision Trees\n",
        "\n",
        "Prone to overfitting\n",
        "Trees can grow too deep, fitting noise rather than patterns (especially visible on small datasets).\n",
        "\n",
        "Sensitive to small changes in data\n",
        "A slight change can drastically alter the structure of the tree.\n",
        "\n",
        "Bias toward features with many levels\n",
        "Features with many unique values may dominate splits.\n",
        "\n",
        "Not as accurate as ensemble models\n",
        "Stand-alone decision trees are weaker than Random Forests or Gradient Boosting models.\n",
        "\n",
        "Piecewise constant predictions in regression\n",
        "Leads to less smooth predictions (a limitation seen in Boston Housing regression).\n"
      ],
      "metadata": {
        "id": "u4nWQEyJnOY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Train a Decision Tree Classifier using the Gini criterion\n",
        "‚óè Print the model‚Äôs accuracy and feature importances\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "DISCMdOwnm5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Feature importances\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Feature Importances:\", importances)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owRmGh4bntxh",
        "outputId": "004cbb31-873e-4198-e9d7-8e18984f4035"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01911002 0.89326355 0.08762643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "QUNSd9KHn9Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Fully-grown Decision Tree\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_pred = full_tree.predict(X_test)\n",
        "full_acc = accuracy_score(y_test, full_pred)\n",
        "\n",
        "# Decision Tree with max_depth=3\n",
        "limited_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "limited_pred = limited_tree.predict(X_test)\n",
        "limited_acc = accuracy_score(y_test, limited_pred)\n",
        "\n",
        "print(\"Accuracy of Fully-Grown Tree:\", full_acc)\n",
        "print(\"Accuracy of Tree with max_depth=3:\", limited_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXjDFTU1n_cy",
        "outputId": "d0402469-fc55-488b-ee8b-e7034a43dfcf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Fully-Grown Tree: 1.0\n",
            "Accuracy of Tree with max_depth=3: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8.Write a Python program to:\n",
        "‚óè Load the Boston Housing Dataset\n",
        "‚óè Train a Decision Tree Regressor\n",
        "‚óè Print the Mean Squared Error (MSE) and feature importances\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:-Below is a complete Python program that:\n",
        "\n",
        "Loads the Boston Housing Dataset (via fetch_openml, since load_boston is deprecated)\n",
        "\n",
        "Trains a Decision Tree Regressor\n",
        "\n",
        "Prints the Mean Squared Error (MSE)\n",
        "\n",
        "Prints feature importances\n",
        "\n",
        "A sample output is included (your exact numbers may vary)."
      ],
      "metadata": {
        "id": "BL6p_a2qoJzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# Decision Tree Regressor on Boston Housing Dataset\n",
        "# ================================================\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load the Boston Housing dataset\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train the Decision Tree Regressor\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predictions and MSE\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# 5. Feature Importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(X.columns, importances):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5uJ2L9Pri7Q",
        "outputId": "798807bb-1cca-4275-b674-768f0a7ce69c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 10.416078431372549\n",
            "\n",
            "Feature Importances:\n",
            "CRIM: 0.0513\n",
            "ZN: 0.0034\n",
            "INDUS: 0.0058\n",
            "CHAS: 0.0000\n",
            "NOX: 0.0271\n",
            "RM: 0.6003\n",
            "AGE: 0.0136\n",
            "DIS: 0.0707\n",
            "RAD: 0.0019\n",
            "TAX: 0.0125\n",
            "PTRATIO: 0.0110\n",
            "B: 0.0090\n",
            "LSTAT: 0.1933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9.Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Tune the Decision Tree‚Äôs max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "‚óè Print the best parameters and the resulting model accuracy\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:-Below is a complete Python program that:\n",
        "\n",
        "Loads the Iris dataset\n",
        "\n",
        "Tunes a Decision Tree classifier‚Äôs max_depth and min_samples_split using GridSearchCV\n",
        "\n",
        "Prints the best parameters and the resulting model accuracy\n",
        "\n",
        "A sample output is also included (your exact numbers may vary slightly due to randomness)."
      ],
      "metadata": {
        "id": "xPSEFfQsqfR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Decision Tree GridSearch on Iris Dataset\n",
        "# ============================\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Set up the Decision Tree and parameter grid\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    \"max_depth\": [1, 2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# 4. GridSearchCV\n",
        "grid = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 5. Best parameters and model accuracy\n",
        "best_params = grid.best_params_\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Model Accuracy on Test Set:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCxnIypDo4C0",
        "outputId": "bf59ee93-4f08-4fa1-ed0f-c48697a2ab7a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy on Test Set: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. Imagine you‚Äôre working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "‚óè Handle the missing values\n",
        "‚óè Encode the categorical features\n",
        "‚óè Train a Decision Tree model\n",
        "‚óè Tune its hyperparameters\n",
        "‚óè Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world setting.\n",
        "Answer:-1) Understand the data (quick checks)\n",
        "\n",
        "Inspect columns, datatypes, cardinalities, target distribution.\n",
        "\n",
        "Check missingness pattern: missing completely at random (MCAR), at random (MAR), or not at random (MNAR). Use df.isna().mean() and visualize with missingness heatmaps.\n",
        "\n",
        "Check class balance ‚Äî disease labels are often imbalanced. Note prevalence (e.g., 2% positive vs 98% negative).\n",
        "\n",
        "2) Handle missing values\n",
        "\n",
        "Goal: avoid biased imputations, preserve signal in missingness.\n",
        "\n",
        "Steps & options\n",
        "\n",
        "Simple exploratory approach\n",
        "\n",
        "If a column is > 60‚Äì80% missing, consider dropping it unless domain knowledge says otherwise.\n",
        "\n",
        "If a row has many missing values, consider dropping only if few and nonrepresentative.\n",
        "\n",
        "Imputation strategies (by feature type)\n",
        "\n",
        "Numerical\n",
        "\n",
        "Mean/median imputation (fast, baseline). Use median when skewed.\n",
        "\n",
        "KNN imputation or IterativeImputer (MICE) for correlated features.\n",
        "\n",
        "Model-based imputation (e.g., LightGBM regressor to predict missing values) for higher fidelity.\n",
        "\n",
        "Categorical\n",
        "\n",
        "Fill with a new category like \"Missing\" (often very effective).\n",
        "\n",
        "Use the most frequent category or a predictive model for imputation.\n",
        "\n",
        "Time/sequence data\n",
        "\n",
        "Forward/backward fill when sequence order matters.\n",
        "\n",
        "Preserve missingness as a feature\n",
        "\n",
        "Add binary indicators like feature_X_missing = 1 if missing ‚Äî often informative (missingness itself may correlate with disease).\n",
        "\n",
        "Practical rule\n",
        "\n",
        "For Decision Trees, imputations like median + missing indicator are a robust, simple start (trees don‚Äôt require scaling).\n",
        "\n",
        "3) Encode categorical features\n",
        "\n",
        "Decision Trees can work with integer-encoded categories but must avoid implying ordinality when none exists.\n",
        "\n",
        "Options\n",
        "\n",
        "Low-cardinality nominal (<= ~10 unique)\n",
        "\n",
        "One-hot encoding (pandas get_dummies, OneHotEncoder) ‚Äî simple; watch dimensionality.\n",
        "\n",
        "High-cardinality\n",
        "\n",
        "Target encoding / mean encoding (with cross-validation or smoothing to avoid leakage).\n",
        "\n",
        "Frequency encoding (replace category by frequency).\n",
        "\n",
        "Hashing (if dimensionality and memory are issues).\n",
        "\n",
        "Ordinal features (true order)\n",
        "\n",
        "Map to integers reflecting order.\n",
        "\n",
        "Important: When using target encoding, use out-of-fold encoding or a holdout to avoid target leakage.\n",
        "\n",
        "4) Feature engineering & selection\n",
        "\n",
        "Create clinically meaningful aggregated features (e.g., BMI from weight/height, time since last test).\n",
        "\n",
        "Interactions: domain-driven interactions may help.\n",
        "\n",
        "Remove or aggregate very sparse categories.\n",
        "\n",
        "Use feature selection if needed (tree feature importances, permutation importance, or regularized models).\n",
        "\n",
        "5) Train a Decision Tree model (baseline)\n",
        "\n",
        "Data split: hold out a test set (e.g., 20‚Äì30%) stratified by target. Keep a separate validation strategy for tuning (CV).\n",
        "\n",
        "Baseline model: DecisionTreeClassifier(criterion='gini', random_state=42) with sensible defaults.\n",
        "\n",
        "Train on training set; compute metrics on validation/test.\n",
        "\n",
        "Important practical notes\n",
        "\n",
        "Trees don‚Äôt require feature scaling.\n",
        "\n",
        "Use pipelines (scikit-learn Pipeline) to ensure consistent preprocessing and to avoid leakage.\n",
        "\n",
        "6) Tune hyperparameters\n",
        "\n",
        "Use cross-validation and either GridSearchCV or RandomizedSearchCV (or Bayesian optimization like Optuna for efficiency).\n",
        "\n",
        "Key hyperparameters to tune\n",
        "\n",
        "max_depth ‚Äî controls tree depth (prevent overfitting).\n",
        "\n",
        "min_samples_split ‚Äî min samples required to split a node.\n",
        "\n",
        "min_samples_leaf ‚Äî min samples in a leaf (important to reduce variance).\n",
        "\n",
        "max_features ‚Äî number of features to consider for split (can improve generalization).\n",
        "\n",
        "criterion ‚Äî 'gini' or 'entropy' (usually small effect).\n",
        "\n",
        "ccp_alpha ‚Äî cost-complexity pruning parameter (scikit-learn supports pruning via this).\n",
        "\n",
        "(If very large data) consider max_leaf_nodes.\n",
        "\n",
        "Example tuning approach\n",
        "\n",
        "Start with RandomizedSearch over wide ranges, then refine with GridSearch around best values.\n",
        "\n",
        "Use StratifiedKFold for classification; use appropriate scoring (see below).\n",
        "\n",
        "If class imbalance exists, tune using metrics that reflect business objectives (e.g., recall for positives).\n",
        "\n",
        "7) Evaluation (metrics & procedures)\n",
        "\n",
        "Train/validation strategy\n",
        "\n",
        "Use nested CV if you want unbiased estimates when hyperparameters are tuned.\n",
        "\n",
        "Always evaluate on a final held-out test set.\n",
        "\n",
        "Metrics to report\n",
        "\n",
        "Accuracy ‚Äî OK if classes balanced (rarely sufficient).\n",
        "\n",
        "Confusion matrix ‚Äî to see FP/FN counts.\n",
        "\n",
        "Precision, Recall (Sensitivity), Specificity ‚Äî recall is often crucial in healthcare (catch cases); precision matters to avoid unnecessary interventions.\n",
        "\n",
        "F1-score ‚Äî harmonic mean when balance between precision & recall is desired.\n",
        "\n",
        "ROC-AUC ‚Äî general discrimination metric (useful even with imbalance).\n",
        "\n",
        "PR-AUC (Precision-Recall AUC) ‚Äî better for highly imbalanced problems (focuses on positive class).\n",
        "\n",
        "Calibration ‚Äî check whether predicted probabilities match observed frequencies (use calibration plots / Brier score). Trees can be poorly calibrated ‚Äî consider calibrators (CalibratedClassifierCV) or use ensemble methods.\n",
        "\n",
        "Decision-curve analysis ‚Äî evaluate net benefit at different threshold (useful for clinical decision-making).\n",
        "\n",
        "Threshold tuning\n",
        "\n",
        "Default 0.5 probability threshold may not match business needs. Tune threshold to balance sensitivity vs specificity based on cost of false positives/negatives.\n",
        "\n",
        "Explainability\n",
        "\n",
        "Extract feature importances (but be careful: raw importances are biased toward splits with many levels).\n",
        "\n",
        "Use SHAP or LIME for local explanations (helpful for clinicians and regulatory requirements).\n",
        "\n",
        "Provide partial dependence plots for top features.\n",
        "\n",
        "8) Handling class imbalance\n",
        "\n",
        "Resampling: oversampling minority (SMOTE), undersampling majority, or combined approaches.\n",
        "\n",
        "Class weights: set class_weight='balanced' in the tree to penalize misclassifying minority class.\n",
        "\n",
        "Use evaluation metrics appropriate to imbalanced classes (PR-AUC, recall).\n",
        "\n",
        "9) Validation, robustness, and fairness checks\n",
        "\n",
        "Robustness: test performance across subgroups (age ranges, hospitals, demographics).\n",
        "\n",
        "Fairness: check for disparate impact and biases; consult domain/regulatory constraints.\n",
        "\n",
        "Stability: run repeated CV folds and report variability (std dev) of metrics.\n",
        "\n",
        "10) Deployment & monitoring\n",
        "\n",
        "Model packaging: export preprocessing + model as a pipeline (pickle/ONNX).\n",
        "\n",
        "Monitoring: monitor data drift, performance drift, and input distributions.\n",
        "\n",
        "Retraining schedule: define triggers (degradation, data drift) or periodic retraining.\n",
        "\n",
        "Clinical validation: require prospective validation or a clinical trial before automated decisions.\n",
        "\n",
        "11) Risk management & governance\n",
        "\n",
        "Document model assumptions, data provenance, and known failure modes.\n",
        "\n",
        "Define escalation: human review thresholds (e.g., when model confidence low).\n",
        "\n",
        "Ensure compliance with health data regulations (e.g., HIPAA equivalents if applicable).\n",
        "\n",
        "12) Business value (how the model helps the company)\n",
        "\n",
        "Early detection / triage ‚Äî identify high-risk patients for further testing, enabling earlier treatment and better outcomes.\n",
        "\n",
        "Resource allocation & cost savings ‚Äî prioritize diagnostic tests and interventions to patients most likely to benefit, reducing unnecessary procedures.\n",
        "\n",
        "Operational efficiency ‚Äî automate initial screening to reduce clinician load; free up specialists for complex cases.\n",
        "\n",
        "Personalized care ‚Äî tailor follow-up and monitoring based on predicted risk.\n",
        "\n",
        "Public health insights ‚Äî aggregate predictions to identify hotspots, plan interventions, and support epidemiological analyses.\n",
        "\n",
        "Quantifying value\n",
        "\n",
        "Reduction in late-stage diagnoses, cost per diagnosis, false negative cost, and clinician time saved are typical KPIs to track.\n",
        "\n",
        "13) Example scikit-learn pipeline snippet (no execution here)"
      ],
      "metadata": {
        "id": "Yc4wNklNpLnL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbb2a60a",
        "outputId": "a6eb87d2-13bf-49fa-c5d9-5fd4acd7ed94"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [None, 3, 5, 7, 10],\n",
        "    'min_samples_split': [2, 5, 10, 15, 20]\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "dtc = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dtc,\n",
        "    param_grid=param_grid,\n",
        "    cv=5, # 5-fold cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1, # Use all available CPU cores\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Parameters found by GridSearchCV:\", grid_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "best_model_accuracy = accuracy_score(y_test, y_pred_best)\n",
        "print(\"Accuracy of the best model on the test set:\", best_model_accuracy)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
            "Best Parameters found by GridSearchCV: {'max_depth': None, 'min_samples_split': 10}\n",
            "Accuracy of the best model on the test set: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14) Final practical tips\n",
        "\n",
        "Start simple (median imputation + missing indicators + one-hot for small cat features) to get a baseline fast.\n",
        "\n",
        "Use domain experts to validate features and define acceptable tradeoffs (e.g., how many false positives can clinicians tolerate?).\n",
        "\n",
        "Prefer conservative thresholds in healthcare where false negatives are costly; always pair model outputs with human review for high-risk decisions.\n",
        "\n",
        "Consider ensemble methods (Random Forest / Gradient Boosting) if you need higher predictive performance ‚Äî but respect interpretability tradeoffs.\n",
        "\n"
      ],
      "metadata": {
        "id": "MKCE2XkNppCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zw37wYEnqZz1"
      }
    }
  ]
}